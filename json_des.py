# dataset_pipeline.py ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —à–∞–±–ª–æ–Ω —Å –ø–æ–ª–Ω—ã–º–∏ –ª–æ–≥–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
import plotly.express as px
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.feature_selection import SelectKBest, f_classif
from statsmodels.stats.outliers_influence import variance_inflation_factor
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
from sklearn.base import clone
from imblearn.over_sampling import SMOTE
from collections import Counter
import os
import warnings
warnings.filterwarnings('ignore')

# === –ó–∞–≥—Ä—É–∑–∫–∞ ===
filename = "iris.json"  # –ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –≤–∞—à —Ñ–∞–π–ª
df = pd.read_json(filename)
print("\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω. –†–∞–∑–º–µ—Ä:", df.shape)

# === –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===
def analyze_feature_types(df):
    print("\nüîç –¢–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    for col in df.columns:
        if df[col].dtype == 'object' or df[col].nunique() < 20:
            print(f"- {col}: –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π ({df[col].nunique()} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö)")
        else:
            print(f"- {col}: —á–∏—Å–ª–æ–≤–æ–π")

analyze_feature_types(df)

# === –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤ ===
print("\nüìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤ (missingno):")
msno.matrix(df)
plt.title("–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
plt.show()

# === –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ===
duplicates = df.duplicated().sum()
df = df.drop_duplicates()
print(f"\n‚úÖ –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates}. –ù–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {df.shape}")

# === –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ ===
print("\nüîÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤:")
for col in df.columns:
    null_count = df[col].isnull().sum()
    if null_count == 0:
        continue
    if df[col].dtype == 'object':
        fill_value = df[col].mode()[0]
        df[col].fillna(fill_value, inplace=True)
        print(f"- {col}: –∑–∞–º–µ–Ω–µ–Ω–æ {null_count} ‚Üí –º–æ–¥–∞ '{fill_value}'")
    else:
        fill_value = df[col].median()
        df[col].fillna(fill_value, inplace=True)
        print(f"- {col}: –∑–∞–º–µ–Ω–µ–Ω–æ {null_count} ‚Üí –º–µ–¥–∏–∞–Ω–∞ {fill_value}")

# === –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===
constant_cols = [col for col in df.columns if df[col].nunique() <= 1]
df.drop(columns=constant_cols, inplace=True)
print(f"\n‚ö†Ô∏è –£–¥–∞–ª–µ–Ω–æ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {constant_cols if constant_cols else '–Ω–µ—Ç'}")

# === –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ —É–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ ===
print("\nüö® –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ (IsolationForest):")
numeric_cols = df.select_dtypes(include=[np.number]).columns
iso = IsolationForest(contamination=0.03, random_state=42)
outliers = iso.fit_predict(df[numeric_cols])
outlier_count = (outliers == -1).sum()
df = df[outliers == 1]
print(f"–£–¥–∞–ª–µ–Ω–æ –≤—ã–±—Ä–æ—Å–æ–≤: {outlier_count}. –†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {df.shape}")

# === –ì—Ä–∞—Ñ–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===
print("\nüìä –ì—Ä–∞—Ñ–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –∏ –±–æ–∫—Å–ø–ª–æ—Ç—ã):")
import math

numeric_cols = df.select_dtypes(include=[np.number]).columns
n_cols = len(numeric_cols)
n_rows = math.ceil(n_cols / 2)

plt.figure(figsize=(15, n_rows * 4))
for i, col in enumerate(numeric_cols):
    plt.subplot(n_rows, 2, i + 1)
    sns.histplot(df[col], kde=True, bins=30, color='skyblue')
    plt.title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {col}')
plt.tight_layout()
plt.show()

plt.figure(figsize=(15, n_rows * 4))
for i, col in enumerate(numeric_cols):
    plt.subplot(n_rows, 2, i + 1)
    sns.boxplot(x=df[col], color='lightcoral')
    plt.title(f'–ë–æ–∫—Å–ø–ª–æ—Ç: {col}')
plt.tight_layout()
plt.show()

for i in df.columns:
    fig = px.histogram(df,
                   x=i,
                   marginal='box',
                   text_auto=True,
                   color_discrete_sequence  = ['forestgreen'],
                   template='simple_white',
                   title=i.upper() + ' HISTOGRAM')

    fig.update_layout(xaxis_title=i,yaxis_title="Count", bargap=0.1)

    fig.show()

# === –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===
def encode_categorical_features(df):
    df_encoded = df.copy()
    label_encoders = {}
    print("\nüß† –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    for col in df.columns:
        if df[col].dtype == 'object' or df[col].nunique() < 20:
            le = LabelEncoder()
            df_encoded[col] = le.fit_transform(df[col])
            label_encoders[col] = le
            print(f"- {col}: –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω (LabelEncoder) ‚Üí {dict(zip(le.classes_, le.transform(le.classes_)))})")
    return df_encoded, label_encoders

df, label_encoders = encode_categorical_features(df)

# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ ===
processed_name = os.path.splitext(filename)[0] + "_–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π.csv"
df.to_csv(processed_name, index=False)
print(f"\nüíæ –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {processed_name}")

# === –ú—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å (VIF) ===
def compute_vif(df_num):
    print("\nüìâ –†–∞—Å—á—ë—Ç –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏ (VIF):")
    from statsmodels.tools.tools import add_constant
    X_const = add_constant(df_num)
    vif = pd.DataFrame()
    vif["Feature"] = df_num.columns
    vif["VIF"] = [variance_inflation_factor(X_const.values, i + 1) for i in range(df_num.shape[1])]
    print(vif)

compute_vif(df[numeric_cols])

# === –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è ===
target_col = 'species'  # ‚Üê –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ—é
X = df.drop(columns=[target_col])
y = df[target_col]

# === –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ ===
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)
print("\nüìè –ü—Ä–∏–∑–Ω–∞–∫–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω—ã (StandardScaler)")

# === SelectKBest ===
selector = SelectKBest(score_func=f_classif, k=10)
selector.fit(X_scaled, y)
top_features = X_scaled.columns[selector.get_support()]
X_selected = X_scaled[top_features]
print(f"\n‚úÖ SelectKBest –≤—ã–±—Ä–∞–ª —Ç–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {list(top_features)}")

# === Exhaustive Feature Search ===
print("\nüîç Exhaustive Feature Search (mlxtend):")
base_model = LogisticRegression(max_iter=1000)
efs = EFS(clone(base_model),
          min_features=2,
          max_features=4,
          scoring='accuracy',
          print_progress=True,
          cv=3)
efs = efs.fit(X_selected, y)
X_selected = X_selected[list(efs.best_feature_names_)]
print(f"‚úÖ –õ—É—á—à–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (EFS): {list(efs.best_feature_names_)}")

# === SMOTE –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ ===
X_bal, y_bal = SMOTE(random_state=42).fit_resample(X_selected, y)
print(f"\n‚öñÔ∏è –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ (SMOTE): –¥–æ {Counter(y)}, –ø–æ—Å–ª–µ {Counter(y_bal)}")

# === –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏ ===
X_train, X_temp, y_train, y_temp = train_test_split(X_bal, y_bal, test_size=0.3, stratify=y_bal, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)
print(f"\nüìÇ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: Train={X_train.shape}, Val={X_val.shape}, Test={X_test.shape}")

# === –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π ===
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(),
    'Gradient Boosting': GradientBoostingClassifier()
}

for name, model in models.items():
    print(f"\nüöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    print(f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ({name}):")
    print(classification_report(y_val, y_pred))

# === –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Random Forest) ===
rf = models['Random Forest']
importances = rf.feature_importances_
sorted_idx = np.argsort(importances)[::-1]
top_feat = X_selected.columns[sorted_idx]

plt.figure(figsize=(10, 5))
sns.barplot(x=importances[sorted_idx], y=top_feat, palette="Blues_d")
plt.title("üåü –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Random Forest)")
plt.tight_layout()
plt.show()

# === –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç ===
print("\nüìå –ò—Ç–æ–≥:")
print("""
‚úîÔ∏è –î—É–±–ª–∏–∫–∞—Ç—ã, –ø—Ä–æ–ø—É—Å–∫–∏, –≤—ã–±—Ä–æ—Å—ã —É—Å—Ç—Ä–∞–Ω–µ–Ω—ã
‚úîÔ∏è –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã
‚úîÔ∏è –í—ã–ø–æ–ª–Ω–µ–Ω–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤
‚úîÔ∏è –ü—Ä–∏–º–µ–Ω—ë–Ω VIF-–∞–Ω–∞–ª–∏–∑ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏
‚úîÔ∏è –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: SelectKBest + Exhaustive Search
‚úîÔ∏è –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —á–µ—Ä–µ–∑ SMOTE
‚úîÔ∏è –û–±—É—á–µ–Ω—ã –º–æ–¥–µ–ª–∏: LogisticRegression, RandomForest, GradientBoosting
‚úîÔ∏è –ü–æ—Å—Ç—Ä–æ–µ–Ω–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
""")